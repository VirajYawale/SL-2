{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (2.32.3)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching latest news...\n",
      "Error fetching news: 404\n",
      "No new articles found.\n",
      "Fetching latest news...\n",
      "Error fetching news: 404\n",
      "No new articles found.\n",
      "Fetching latest news...\n",
      "Error fetching news: 404\n",
      "No new articles found.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 73\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     update_dataset()\n\u001b[0;32m---> 73\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m60\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "# RapidAPI credentials\n",
    "RAPIDAPI_KEY = \"8d4c87454fmsh34632789d84864ap1c1b12jsn9b500e1e3ebb\"  # Replace with your API key\n",
    "RAPIDAPI_HOST = \"google-news13.p.rapidapi.com\"\n",
    "\n",
    "# API Endpoint\n",
    "URL = \"https://google-news13.p.rapidapi.com\"\n",
    "\n",
    "# Query Parameters (Modify as needed)\n",
    "PARAMS = {\n",
    "    \"country\": \"us\",  # Change for other countries (e.g., 'in', 'gb')\n",
    "    \"category\": \"technology\",  # News category (e.g., 'sports', 'health')\n",
    "}\n",
    "\n",
    "# File to store news dataset\n",
    "CSV_FILE = \"real_time_news.csv\"\n",
    "\n",
    "def fetch_news():\n",
    "    \"\"\"Fetch latest news articles from the API.\"\"\"\n",
    "    headers = {\n",
    "        \"X-RapidAPI-Key\": RAPIDAPI_KEY,\n",
    "        \"X-RapidAPI-Host\": RAPIDAPI_HOST\n",
    "    }\n",
    "    \n",
    "    response = requests.get(URL, headers=headers, params=PARAMS)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json().get(\"articles\", [])\n",
    "    else:\n",
    "        print(f\"Error fetching news: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "def update_dataset():\n",
    "    \"\"\"Fetch latest news and update the dataset.\"\"\"\n",
    "    print(\"Fetching latest news...\")\n",
    "    articles = fetch_news()\n",
    "\n",
    "    if not articles:\n",
    "        print(\"No new articles found.\")\n",
    "        return\n",
    "    \n",
    "    # Convert API response to DataFrame\n",
    "    df_new = pd.DataFrame(articles)[[\"title\", \"description\", \"url\", \"publishedAt\", \"source\"]]\n",
    "    df_new[\"source\"] = df_new[\"source\"].apply(lambda x: x[\"name\"])  # Extract source name\n",
    "    \n",
    "    # Check if CSV file exists\n",
    "    if os.path.exists(CSV_FILE):\n",
    "        df_existing = pd.read_csv(CSV_FILE)\n",
    "        \n",
    "        # Avoid duplicates by removing already saved articles\n",
    "        df_new = df_new[~df_new[\"title\"].isin(df_existing[\"title\"])]\n",
    "\n",
    "        if df_new.empty:\n",
    "            print(\"No new updates. Dataset is up to date.\")\n",
    "            return\n",
    "        \n",
    "        # Append new articles to existing dataset\n",
    "        df_new.to_csv(CSV_FILE, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        # Create new dataset file\n",
    "        df_new.to_csv(CSV_FILE, index=False)\n",
    "\n",
    "    print(f\"Dataset updated! {len(df_new)} new articles added.\")\n",
    "\n",
    "# Run script continuously every 60 seconds\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        update_dataset()\n",
    "        time.sleep(60)  # Wait 60 seconds before fetching again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📡 Fetching latest news...\n",
      "❌ Error fetching news: 403\n",
      "ℹ️ No new articles found.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 73\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     update_dataset()\n\u001b[0;32m---> 73\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m60\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response Status Code: 200\n",
      "Error: API returned invalid JSON.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 69\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m     fetch_and_store_news()\n\u001b[0;32m---> 69\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(INTERVAL)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# API Configuration\n",
    "API_KEY = \"37eaf5fe6f85b63687fd531a31428cd3\"  # Replace with your NewsAPI key\n",
    "URL = \"http://api.mediastack.com\"\n",
    "CSV_FILENAME = \"realtime_news_data.csv\"\n",
    "INTERVAL = 300  # Fetch data every 5 minutes\n",
    "\n",
    "params = {\n",
    "    \"country\": \"us\",\n",
    "    \"apiKey\": API_KEY\n",
    "}\n",
    "\n",
    "def fetch_and_store_news():\n",
    "    \"\"\"Fetch real-time news and append it to a CSV file.\"\"\"\n",
    "    response = requests.get(URL, params=params)\n",
    "\n",
    "    print(\"Response Status Code:\", response.status_code)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        try:\n",
    "            # Check if response is empty\n",
    "            if not response.text.strip():\n",
    "                print(\"Warning: API returned an empty response.\")\n",
    "                return\n",
    "\n",
    "            # Try to parse JSON\n",
    "            data = response.json()\n",
    "\n",
    "            # Check if \"articles\" exist\n",
    "            if \"articles\" not in data:\n",
    "                print(\"Error: No 'articles' key in response.\")\n",
    "                return\n",
    "            \n",
    "            articles = data[\"articles\"]\n",
    "\n",
    "            if not articles:\n",
    "                print(\"No new articles found.\")\n",
    "                return\n",
    "\n",
    "            # Convert \"source\" dictionary into a name column\n",
    "            for article in articles:\n",
    "                article[\"source\"] = article[\"source\"][\"name\"]\n",
    "\n",
    "            df = pd.DataFrame(articles, columns=[\"title\", \"description\", \"url\", \"publishedAt\", \"source\"])\n",
    "\n",
    "            # Append new data to the CSV file\n",
    "            df.to_csv(CSV_FILENAME, mode=\"a\", index=False, encoding=\"utf-8\", header=not pd.io.common.file_exists(CSV_FILENAME))\n",
    "\n",
    "            print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] Data updated successfully.\")\n",
    "\n",
    "        except requests.exceptions.JSONDecodeError:\n",
    "            print(\"Error: API returned invalid JSON.\")\n",
    "\n",
    "    elif response.status_code == 401:\n",
    "        print(\"Error: Unauthorized. Check if your API key is valid.\")\n",
    "    elif response.status_code == 403:\n",
    "        print(\"Error: Access Forbidden. Your API key might be blocked.\")\n",
    "    elif response.status_code == 429:\n",
    "        print(\"Error: Rate limit exceeded. Reduce request frequency.\")\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "\n",
    "# Run the fetch function in a loop for real-time updates\n",
    "while True:\n",
    "    fetch_and_store_news()\n",
    "    time.sleep(INTERVAL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1676247699.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[15], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    pip install feedparser\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import requests\n",
    "import feedparser\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "# Set up your API key (Get it from https://newsapi.org/)\n",
    "NEWS_API_KEY = \"ff190a48a285413e921b5a91343dd0cd\"\n",
    "\n",
    "# Function to fetch news from NewsAPI\n",
    "def fetch_newsapi_news(query=\"latest\", sources=\"bbc-news,cnn\", language=\"en\"):\n",
    "    url = f\"https://newsapi.org/v2/everything?q={query}&sources={sources}&language={language}&apiKey={NEWS_API_KEY}\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    \n",
    "    articles = []\n",
    "    if data.get(\"articles\"):\n",
    "        for article in data[\"articles\"]:\n",
    "            articles.append({\n",
    "                \"source\": article[\"source\"][\"name\"],\n",
    "                \"title\": article[\"title\"],\n",
    "                \"description\": article[\"description\"],\n",
    "                \"url\": article[\"url\"],\n",
    "                \"published_at\": article[\"publishedAt\"]\n",
    "            })\n",
    "    return articles\n",
    "\n",
    "# Function to fetch news from RSS Feeds\n",
    "def fetch_rss_news(feed_url=\"https://rss.cnn.com/rss/edition.rss\"):\n",
    "    feed = feedparser.parse(feed_url)\n",
    "    articles = []\n",
    "    for entry in feed.entries:\n",
    "        articles.append({\n",
    "            \"source\": \"CNN RSS\",\n",
    "            \"title\": entry.title,\n",
    "            \"description\": entry.summary,\n",
    "            \"url\": entry.link,\n",
    "            \"published_at\": entry.published\n",
    "        })\n",
    "    return articles\n",
    "\n",
    "# Combine news from both sources\n",
    "def fetch_all_news():\n",
    "    news_data = []\n",
    "    \n",
    "    # Fetch from NewsAPI\n",
    "    news_data.extend(fetch_newsapi_news(query=\"breaking news\"))\n",
    "\n",
    "    # Fetch from RSS feed\n",
    "    news_data.extend(fetch_rss_news())\n",
    "\n",
    "    return news_data\n",
    "\n",
    "# Save news to CSV\n",
    "def save_news_to_csv(news_data, filename=\"news_data.csv\"):\n",
    "    df = pd.DataFrame(news_data)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"✅ News saved to {filename}\")\n",
    "\n",
    "# Run the script\n",
    "if __name__ == \"__main__\":\n",
    "    news = fetch_all_news()\n",
    "    if news:\n",
    "        save_news_to_csv(news)\n",
    "    else:\n",
    "        print(\"No news found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
